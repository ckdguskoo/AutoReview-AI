**개발 개획서 (초안)**
- **프로젝트명**: AI 자동 코드 리뷰 & PR 평가 시스템
- **목표**: 코드 품질/리스크/테스트 누락을 자동 분석하여 PR 단위로 점수화, 코멘트 생성, 승인/차단 정책 지원
- **핵심 가치**: 리뷰 비용 절감, 릴리즈 안정성 향상, 리뷰 기준의 일관성 확보

**1) 배경 및 필요성**
- 리뷰 편차로 인한 품질 불균형
- 릴리즈 속도 vs 품질 간 균형 문제
- 반복적인 스타일/규칙 리뷰 부담 증가

**2) 범위(Scope)**
- **MVP**
  - PR 메타데이터 수집(파일/라인/커밋/테스트 등)
  - 변경 코드 분석(정적 분석 + LLM 기반 리뷰 코멘트)
  - 위험도/품질 점수 산정
  - 리뷰 요약 리포트 생성
- **확장 범위**
  - 조직별 정책/규칙 커스터마이징
  - CI/CD 연동(자동 차단/승인 룰)
  - 과거 PR 기반 학습/피드백 루프

**3) 기능 요구사항**
- **PR 수집**: GitHub/GitLab/Bitbucket API 연동
- **정적 분석**: lint, SAST, 커버리지, 복잡도
- **LLM 리뷰**:
  - 버그/성능/보안 관점의 지적
  - 테스트 누락 감지
  - 코딩 컨벤션 및 설계 개선 제안
- **스코어링**:
  - 위험도(High/Med/Low)
  - 코드 품질 점수(0~100)
- **리포트**:
  - 요약/주요 이슈
  - 파일별 상세 코멘트
- **정책**:
  - 규칙 기반 승인/차단
  - 예외 정책(화이트리스트)

**4) 비기능 요구사항**
- 응답 시간: PR 기준 1~5분 이내
- 확장성: 병렬 분석 가능
- 보안: 소스코드 저장 최소화, 암호화

**5) 시스템 구성(개념)**
- **Collector**: PR 데이터 수집
- **Analyzer**: 정적 분석 + AI 모델
- **Scoring Engine**: 평가 기준 적용
- **Report Generator**: 리뷰/요약 출력
- **Policy Gate**: 승인/차단 결정

**6) 기술 스택(예시)**
- Backend: Python/FastAPI
- Worker(선택): Celery + Redis
- LLM: OpenAI API / 로컬 모델
- Static Analysis: ESLint, SonarQube
- Storage: PostgreSQL + Redis
- CI 연동: GitHub Actions

**7) 일정(예시)**
- 1~2주: 요구사항 정리, 프로토타입
- 3~5주: MVP 기능 개발
- 6~8주: CI 연동, 대시보드, 안정화

**8) 리스크 및 대응**
- 모델 평가 품질 편차 → 룰 기반 병행
- 민감 코드 유출 우려 → 온프레미스 옵션
- 리뷰 신뢰성 → 휴먼 검증 루프

**9) 성공 지표(KPI)**
- 리뷰 시간 30% 이상 감소
- 버그 회수율 20% 이상 감소
- 리뷰 코멘트 재사용율 향상
